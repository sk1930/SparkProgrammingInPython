50. Spark Data Sources and Sinks


Welcome Back. We dedicate this section for Spark Data Sources and Sinks. In this first lecture, I am going to give you a brief introduction to the Spark Sources and Sinks. The rest of the section will help you learn them in more detail. So, let's start. I already introduced you to the Data Lake concept, and you learned that the Spark is used for processing large volumes of data. Right? However, any processing engine, including Spark, must-read data from some data source. And that's what we mean by Spark Data Sources. These data sources can be further categorized into two groups. External Data Sources and Internal Data Sources Now let's try to understand them from a Data Lake perspective. Your data might be stored in some source systems such as Oracle or SQL Server Databases. It might be stored at some application servers, such as your application logs. However, all these systems are external to your Data Lake. You don't see them in this Data Lake conceptual diagram. So, we categorize such data sources as external data sources. The list of possible external data sources is too long. However, here are some notable systems. JDBC Data Sources such as Oracle, SQL Server, MySQL, PostgreSQL etc. No SQL Data Systems such as Cassandra, MongoDB Cloud Data Warehouses such as Snowflake, Redshift Stream Integrators such as Kafka Now the question is this How can we read data from external sources to Spark? You cannot process the data from these systems unless you read them and create a Spark DataFrame or a Dataset. Make sense? There are two approaches. The first approach is to bring your data to the Data Lake and store them in your Lake's distributed storage. How you bring it to the lake is your choice. The most commonly used approach is to use a suitable data integration tool.

The second approach is to use Spark Data Source API to directly connect with these external systems. And Spark allows you to do it for a variety of source systems. For example, you can connect to all of these, which I have listed here. So now you have two choices. Which one do you prefer?
I prefer the first one for all my batch processing requirements and the second one for all my stream processing requirements. However, stream processing is not in the scope of this course, so I leave that discussion for another course. Now let's assume that I have a batch processing requirement. So I am going to prefer the first approach Using a data integration tool to bring data to the distributed storage, and then we start processing it.
Why do I prefer this two-step approach? Here are some reasons.
Modularity, load balance, security, flexibility

Modularity :
Bringing data correctly and efficiently to your lake is a complex goal in itself. We want to decouple the ingestion from the processing to improve manageability.

load balance:
Your source system would have been designed for some specific purpose. And the capacity of your source system would have been planned accordingly. Right? Now, if you want to connect your Spark workload to these systems, then you must have to replan your source system capacity and the security aspects of those systems.
flexibility:  We want to use the right tool for the right purpose. A spark is an excellent tool for data processing. However, It wasn't designed to handle the complexities of Data Ingestion. So, most of the well designed real-life projects are not going to directly connect to the external systems even though we can do it.

Now let's come to the second category - Internal Data Sources. So your internal data source is your distributed storage. It could be HDFS or cloud-based storage(Amazon S3, Azure, Google cloud.)
However, at the end of the day, your data is stored in these systems as a data file. The mechanics of reading data from HDFS or from cloud storage is the same.

However, the difference lies in the data file format. Here are some commonly used file formats: CSV, JSON, Parquet, AVRO, Plain text.  Other than these file formats, we also have two more options - Spark SQL Tables and Delta Lake. These two are also backed by data files. However, they also include some additional metadata that is stored outside the data file. So we do not categorize them as file sources. Great! So now you understand the Spark Data Sources part.


What is the Data Sink? The data sinks are the final destination of the processed data. So you are going to load the data from an internal or an external source. Then you will be handling it using the Spark APIs. Once your processing is complete, you want to save the outcome to an internal or an external system. And these systems could be a data file in your data lake storage, or it could be an external system such as a JDBC database, or a NoSQL database. Right? So the idea remains the same. Working with the data source is all about reading the data, and working with the sink is about writing the data.
Same as sources, Spark allows you to write the data in a variety of file formats, SQL tables, and delta-lake. Spark also allows you to directly write the data to a bunch of external sources such as JDBC databases, Cassandra, and MongoDB. However, we again do not recommend directly writing data to the external systems for the same reasons as we do not directly read from these external systems. Great! So now you understand the basics of Spark Data Sources and Sinks. In the rest of the section, I am going to create some examples and help you learn the mechanics of working with internal sources and sinks. External systems are beyond the scope of the course. See you again. Keep Learning and Keep Growing.