63. Creating and Using User Defined Functions





Welcome Back. In the previous lecture, we learned to refer columns and create expressions. I showed you examples to create expressions using the following two methods. String Expressions or SQL Expressions and Column Object Expressions We already learned to refer Dataframe columns, applying some maths, and using built-in functions. However, Spark also allows you to create user-defined functions and use them in these two types of expressions. In this lecture, we will learn to create UDF and use them in our expressions. So, let's start. I created this UDFDemo example. I have this sample data file. I am creating a Spark session and loading this data file into a Dataframe. Let me show you the Dataframe. Good. Can you see this gender column? We have a different kind of text here. However, I want to standardize it to one of the following values. Male Female and Unknown .

How to fix it? We have many ways to do this transformation. However, I want to create a custom function to fix this problem. So, let's define the function. Let me call it the parseGender function. This function takes a string and returns a string. So, I intend to use this function where I will pass the gender column, and it should return one of the desired values, i.e., male, female, or Unknown. Let's create the function body. I am going to use a regular expression to infer gender. Here is the regular expression for females. And here it is for the male. What does it mean? I have three patterns to infer the female gender. And all those three patterns are combined with an OR condition. Let me quickly explain these patterns The string is a single character 'f.' It contains f followed by some character and letter m. It contains w followed by some other letter and m. Similarly, the pattern for the male gender is also designed. These patterns may not be a perfect gender match pattern, but it works for the given data. Now let's test it. Here is the code. If we can find the lower case of the string in the female pattern, then it is a female. Similarly, we check for male else we return Unknown. Good.

So my function is now ready. How do I use it? We can create an expression and use it. However, we have two approaches to develop the expression Column Object Expression And String Expression Let's learn both. The first one is the Column Object Expression. I am going to create a new Dataframe transforming the existing Dataframe.

And I am going to use the withColumn() transformation. What is withColumn()? And why am I using it? Let me explain. The withColumn() transformation allows you to transform a single column without impacting other columns in the Dataframe. It takes two arguments. The first argument is the column name that you want to transform. So, in my case, it is the Gender column. Right? We want to transform the gender. The next argument is a column expression. So, what I want to do is to use the parseGender() function and supply the Gender column. My parseGender() function will fix the gender string and return a standardized gender string. Right? That's all we want to do here.

However, we cannot simply use a function in a Column Object Expression. I need to register my custom function to the driver and make it a UDF. Let me do it. I am going to use the UDF() function to register it. So, let me import the functions. Now, I can use the UDF function to register my Python function using the name of my local function. You can also specify the return type of your function. The default return type is StringType. So, the return type is optional if your function returns a String. However, let me specify it. That's all.

The UDF() function will register it and returns a reference to the registered UDF. That's all. Let me show the new Dataframe. So, we can see the before and after state. So what exactly are we doing?

It is a three-step process to use a user-defined function. Create your function. Register it as UDF and get the reference. Now your function is registered in the Spark Session. And your driver will serialize and send this function to the executors. So, executors can smoothly run this function. Right? Finally, use your function in your expression. That's all. Great! So we learned to create and use a UDF in a Column Object expression.


Can we use it in a string or SQL expression? Yes, we can. However, the registration process is different. We need to register it as a SQL function, and it should go to the catalog. And that is done using Spark Session UDF registration method. The first argument is the name of the UDF, and the second argument is the signature of your function. So, these two registrations are different. The first one is to register your function as a Dataframe UDF. This method will not register the UDF in the catalog. It will only create a UDF and serialize the function to the executors. The second type of UDF registration is to register it as a SQL function. This one will also create one entry in the catalog. If you want to use your function in a Dataframe column object expressions, then you must register it like this.

parse_gender_udf = udf(parse_gender,StringType())


But if you want to use your function in a SQL expression, then you must register it like this. Let me query the catalog after the first kind of registration. So, we use spark-catalog and get a list of all functions in the catalog. Then we loop through the list. And check if the parse gender is there in the function name. If yes, we print it. That's all. So this code is a Python List Comprehension, and I am assuming you understand it. Right? This code is going to print the function details if it is available in the catalog. However, as I said earlier, the UDF() function doesn't create a catalog entry. So, we should not be getting any output. However, if we place the same code after the second type of registration, we expect it to find an entry and print the function details. Right? Great! Now let's come to the expression creation. I am going to create an equivalent SQL expression. I will copy this line and modify it. So, the first thing is to place a double quote around the expression and make it a string. However, we have a small problem. The withColumn() doesn't take a SQL expression. But that's not a big problem, right? We can use the expr() function here. You already learned it in the earlier video. Now, the Spark engine will parse this string at runtime and resolve the UDF and the column name from the catalog. Right? That's all. Let me run it and show you the outcome. Good. So the first output is showing the original data frame. You can see this M in the gender string. Right? Then we have the catalog entry. We registered parse_gender() using the UDF() function, but we don't see it in the catalog. The following output is for the transformed Dataframe. You can see fixed gender strings. Right? Then we again see the catalog entry. This time you will find your UDF here. Finally, you will see the new Dataframe. We used the withColumn() transformation, which affected only the Gender field. The rest of all fields remained unchanged. Make Sense? Great. That's all for this video. Keep Learning and Keep Growing.
