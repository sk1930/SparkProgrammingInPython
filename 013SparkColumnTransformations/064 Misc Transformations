64. Misc Transformations

Welcome Back. We learned a bunch of transformation tools. However, I still have a few more commonly used techniques to cover. In this video, I am going to cover the following things. We used PyCharm IDE for many of our examples. We used PyCharm IDE for many of our examples. However, we still need to become a little more familier with the local Jupyter Notebook. So, let's do this example with the Jupyter notebook. The notebook is included in your GitHub repository. You can refer it from there. I also have a Pycharm project for the same. If you wanted to do this in the PyCharm IDE, you can refer to the PyCharm project in the GitHub Repository. Great! So, let's start with the first item Quick method to create DataFrames I created this MiscDemo example, which is structured like the earlier examples. I have already created this SparkSession. Now I want to programmatically create a DataFrame. We already learned to create a DataFrame on the fly and also learned that we use them in unit testing our code. Right? However, we also have a quick and dirty method for creating a Dataframe. Let me show you. Let me create a list with some sample data elements. So, I am creating five sample records, each having four fields. The first field is the name, then we have a day, month, and year. Some of the year fields are four-digit year, and others are only two digits. We have a duplicate record also in this sample data set. We will learn to fix all these problems and also learn a few other things. But before that, we need a data frame. Right? So let me create a new DataFrame using the data list. I am using the CreateDataFrame() method without any schema. Right? So this data frame is actually a dataset of four columns. The data type of the column is automatically inferred from the list. However, we do not have a meaningful column name because we didn't attach a schema. But I want to connect a schema to it. How to do it? namedtuple. Right? That's the only method we learned. You can definitely define a namedtuple and apply it here. However, we also have a quick method to attach the column names. You can use the toDF() method and give a list of column names. That's all. Now, this DataFrame is as good as we created in the earlier lecture for unit testing our function. In this method, we skipped multiple things like parallelizing your data, creating RDD, creating schema definition, etc. This approach could be more convenient for quickly creating a data frame for testing or exploring some techniques. Make Sense? Good. You can print the schema of this data frame and quickly check it. Let me run it. So, all these fields are integer, which is good. However, I wanted to show you some specific problems dealing with incorrect data types. So, let me come back to the example and change these fields to string. Re-run it. Now, these fields are inferred as a string. Right? Great! So you learned to quickly create DataFrame and adjust their data types. Let's move on to the next item. So, I have this data frame, and I want to add one more field in this data frame to create a unique identifier for each record. You can do it using a built-in function monotonically increasing id. The function generates a monotonically increasing integer number, which is guaranteed to be unique across all partitions. However, don't expect a consecutive sequence number. It could be any integer number, but you won't get a duplicate. I recommend that you look at the list of available built-in functions at least once and learn what tools do you have in your hand. I already shared the documentation link in the earlier video. Let's add an id filed. But before that, let me repartition this data frame and make three partitions. While working on a local machine, we always make sure that we have more than one partition so we can sense a realistic behavior. However, you should remove these unnecessary partitions from your final code. Good. Now I will create a new data frame and use the withColumn() transformation. We already learned to use the withColumn() for transforming an existing column. You can also use this method to add a new column in the data frame. So, I am going to transform the column id. Since column id does not exist in the data frame, it will be added. And the value of the column will come from the monotonically_increasing_id function. That's all. Let me run it and show you. So, I have a new column, and the values are filled by the function call. Make sense? Good. Let's move to the next item. The Case When Then is a popular construct in programming languages. It is commonly known as switch case, and we prefer to use it to avoid lengthy if-else statements. It is trendy among SQL developers as well. I want to use it to fix the year digit problem. So, I have these two-digit years, and I want to make it four-digit. Well, it is almost impossible to fix it correctly, but we can make some assumptions to correct it. Let's assume that the year between 0 to 20 has an intention to be a post-2000 year. And anything between 21 to 99 is in the previous century. So, this one should be 2006, and the other two are the 1981 and 1963. This one is a duplicate. Can create a case expression for this. Let's do it. I will add one more withColumn() to work with the year filed. Then I am going to create an expression using a multi-line string. Good. So now, I can write the case expression. It is as simple as the case when year < 21 then take year + 2000 When year < 100, then take year + 1900 else take the year without any change. Close the case with the end. Make sense? Let's run it. Do you see any problem? The year becomes decimal. This is caused by the incorrect data type and the automatic type promotion. The year field in the data frame is a string. However, we performed an arithmetic operation in the year field. So, the Spark SQL engine automatically promoted it to decimal. And after completing the arithmetic operation, it is again demoted back to string because the data frame schema is for a string field. However, in all this, we still keep the decimal part, and that's how you see a decimal value in the result. How to fix it? There comes the next item. Casting your data frame columns is a common requirement.

In this example, we have such a need. We have a bunch of ways to do it. However, I am particularly interested in two commonly used approaches. The first option is to make sure that the Spark doesn't automatically promote or demote your field types. Instead, we can write code to push it to an appropriate type. Let me do it. Simple. I am casting the year to an int. Same thing here also. Now the Spark SQL engine does not need to promote it and cause problems for us. This is the recommended approach for casting your fields. Cast it right at the place you want to cast. Let me run it and show you the result. Good. Now you see four-digit years.




Changing your schema is a choice that you need to make. In my case, changing the year field to an integer is perfectly fine. In fact, I should have fixed the data types in the beginning and avoided this casting altogether. Right? Let's do it the right way. So, I am assuming that my initial data frame had an incorrect schema. Now I want to fix the types for the day, month, and year. So, I am going to cast all the three fields. That's all. Once I have the appropriate types, I can do all these arithmetics correctly. Make sense? Good. So you learned two common approaches to fix your data frame schema. Both the methods are good, but all you need to remember is the incorrect types might give you some unexpected results. So, make sure you understand your data frame types and the kind of operations that are allowed. The explicit casing is always a good option to avoid unexpected behavior. Great! Before we move on to the next item, let me show you an alternative method for the implementation case expression. SQL like expression is a more convenient approach, and I personally prefer it. However, we also learned to build our expressions using columns and functions. Right? So let me show you the column object expression for the case expression. We are going to use the same withColumn() method and work with the year field. Now I am going to implement these two when conditions. So we will be using the when() function. The first argument is the condition, and the second argument is the return value. So, when column year is less than 21, then return the column year plus 2000. Make sense? Let's add the second condition. When column year is less than 100, then return the column year plus 1900. Otherwise, return the column year. Simple. Make sense? So you have both the options. You can use whatever you feel natural for you. Great! Now the next item in our list is Adding and removing columns. We already learned to add a new column. We did it using monotonically_increasing_id. Right? I want to add one more column combining the day, month, and year. Let's do it. We again use the withColumn(). Create a new field for the date of birth. Create an expression to concat all the three fields and a field separator. However, this is going to give me a string. And I want to make it a date filed. So, I can use the to_date() function. Where to use the to_date? Well, you can use it inside your SQL expression. Like this. Or, you can use it on the column. Both options are valid and work in the same way. Let me run it. Good. So we have the dob field. However, these three fields are now useless. Right? Can I drop them from my data frame? Yes, we can. Let me do it. Drop the day, month, and year. That's all. Let me rerun it. Looks good. Right? However, if we consider name and dob, then we have one duplicate record. Can I drop the duplicate? Yes, we can. Let me do it. Drop duplicates using name and dob. Let me rerun it. Looks good now. Right? Can I sort it by dob in descending order? Yes, we can. Let me sort it using the dob. By default, it is ascending. However, we wanted to sort it using descending. Well, that is simple. But now, this string is no longer a filed name. It became an expression. So, I will apply the expr() function. Make Sense? Let me run it. Looks good. Right? Great! That's all for this video. Keep learning and Keep Growing.