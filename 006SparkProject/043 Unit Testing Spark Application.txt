43. Unit Testing Spark Application

Welcome Back. So far, in this section, we created a simple Spark Application. We are almost done with the example except for one last thing. Unit testing our application. And that's what we will learn in this video. So, let's start. Here is the example code, and we want to create Unit Test cases for this example. Right? But what do you want to test here? I have two functions. The first one is to load the data file and create a DataFrame. So I can think of at least one test case such as this. Make sure that we are loading the data file correctly. So, we are going to read the data file and validate the number of records. We are going to do this in a minute. But we also have another critical function. This function is implementing some business rules and finally going to give us the count by the country. Right? So, let me create one more test case here. I am going to read the data file, apply the transformations, and validate the results. Great! So now we have two unit test cases to implement. Let's do it. Let me create a new python file. I will name it test_utils.py You can give another name. But both of my functions are in utils.py so let me name it as test_utils.py I will use the Python unit test framework, so let me import the TestClass from the unit test package. You will also need PySpark, so let me import it. I am going to test two of my functions, so let me import them as well. Good. So we are now ready to create our test cases, and I am assuming you are familiar with the Python unit test framework. So we create a class. Let me name it UtilsTestCase, and we inherit the TestCase class from the unit test framework. Good! I am planning to test these two functions. Right? Both of these guys require a spark session. So, I am going to use the class method setUpClass. The setUpClass method executes before all my unit test cases, and hence this one is the best place to set up my SparkSession. So, let me initialize the SparkSession. You already learned to create a new SparkSession. Right? I am going to hardcode the required configurations here. Done. Now I have the spark session.   Great! We also have another class method - tearDownClass. And this one is the best place for your cleanup. All I have is the spark session, which I want to stop. However, I am commenting on this part because I keep getting the Py4J socket error on my machine. Spark session is anyway auto closed. Now we are ready to implement our test cases. Great! Now we can create the first unit test case. Call the function and get your DataFrame. We want to count the records. Right? So, let me get the count. Then, We assert it. That's all. We are done with the first test case. Let's do the next one. I am going to load the data file and then invoke my function. So now, I have a DataFrame that contains my outcome. It should be something like this. Right? But I am not sure about the order of these records, so it is difficult to validate it directly with the DataFrame or to use a List. However, validation could be an easy thing if I convert this DataFrame to a python dictionary. Let me do it. I am going to create an empty python dictionary. Now, I will loop through this list and fill the dictionary with the country name and count. So, the country becomes the key, and the count becomes the value. Right? Great! Now the assertion is straightforward. United States is four, Canada = 2, and the UK has got 1 record. Done. You want to run it? Let's do it. Great! Two of the two tests are passed. That's all for this video. See you again. Keep Learning and Keep growing.