42. Understanding your Execution Plan


In this video, We are going to understand how the application code translates to these
jobs, stages, and tasks. So, let's start. Let me start with the example code and see
how things are build-up, taking a step by step approach. I am going to comment-out
these three lines of code and also comment-out these options. Good. So now we are doing
just one thing.
 return spark.read \
        .csv(data_file)


 Load a data file. We are not inferring the schema. Just a plain data
load. Loading data into a DataFrame is an action, and every Spark action is translated
into a Spark job. Let me execute this example and show you. Great! So we have only one
action in our code, and hence we have one Spark Job. This job is caused by CSV action,
and you can see it here. Now every job must have at least one stage, and each stage
must have at least one task. And that's what we see here. You can see the same in the
stages tab. Each stage comes with its own DAG of internal operations. You can get the
DAG by clicking the stage in the Spark UI. Here it is. So the DAG shows the sequence of
internal processes. This sequence is the compiled code generated by the Spark.
Remember, I told you that Spark is a kind of compiler that compiles your code to some
internal low-level Spark code. And that's what you see here in this DAG. You might not
know the exact meaning of these low-level operations unless you understand the lowest
level of spark core APIs. And you don't even need to know them. However, you can make
some good sense by reading it. So all we are doing here is to scan the file details and
map the partitions. So at this stage, we know which file we want to read and how many
partitions are there in this file. And that's what this high-level code is all about.
Right? That's what we wanted to do. Right? Now let's stop this application and move to
the next step.

Let me uncomment these options. What do you expect? These options are to
make sure that we actually read a portion of the file and infer the column names and
data types for each column. Since we are reading the physical data, you can expect one
more action. Right? One more action should result in one more job. Why? Because each
action translates to a Spark Job. Let's run it and recheck our Spark UI. Good. So now
we have two Spark Jobs. As expected, right? Both jobs are triggered by the CSV method.
Because the high-level code is the CSV method, which internally causes two actions. The
first action is to learn about the file and the partitions. The second action is to
infer the file schema. Right? Each job should have at least one stage and one task. So
we have two stages with one task each. Make sense? Let me check out the DAG for
stage-1. Here it is. It looks like a complicated sequence. But I can easily make out
that we are deserializing the content after scanning the text file. So, we are
inferring the schema while we deserialize it. Right? Great! So now you understand these
two jobs. Both are caused by the spark dot read and the CSV method with these two options. Make Sense?

Now I am going to uncomment everything else. Great! Now before we
jump over the Spark UI, let me explain a few things. So, the spark is going to look at
your code and break it into smaller sections separated by an action. I have two
actions. Reading a file and collecting the results. Right? Now each section becomes at
least one Spark Job. Reading a file and inferring schema are two internal actions. So,
we saw two jobs for this part of the code. However, collecting the results is a simple
action, so I am going to get one job for this section of the code. And this job should
cover everything from here to the collect(). So all these transformations, including
repartition, select, where, groupBy, and count. All of this are planned as a single
job. triggered by the collect action.
Now let's drill down to the Spark Job. Spark will
create a DAG for each job and break it into stages separated by a shuffle operation.
So, here is my DAG for the collect() job. Right? We start from reading the previous
DataFrame, then repartition it, filter, select, groupBy, and finally count it. Do you
know which of these will cause a shuffle? Repartition and groupBy. Right? So Spark is
going to break this job into three stages. This first stage will read the earlier
DataFrame, make two partitions, and write them to an internal buffer called the
exchange. The next stage is going to execute in parallel. Why? Because now we have two
partitions. Each parallel stage task is going to read one partition from the exchange
and perform these narrow transformations. That's what we call Tasks.
Now the second
stage finishes at the groupBy writing the result to exchange. Why? Because we must
shuffle/sort the groupBy results.
The third stage will again start to read the result
from the exchange. And this stage is also going to have two parallel tasks because we
configured our session to have only two partitions for an internal shuffle. Finally,
the collect runs at the driver to collect outcomes from both these tasks. So, in
summary, each action will result in a Job. Each wide-transformation will result in a
separate stage. Right? And every stage executes in parallel depending upon the number
of DataFrame partitions. The first stage operated on a single partition DataFrame, so
we do not have any parallel processing here. The next two stages worked on two
partitions, so we have two parallel tasks. You can see all this in your Spark UI. Here
it is. We have three jobs. The first two jobs are for CSV action. The last job is
collect-action. If you look at the DAG for the whole job, you will see three stages.
The first stage starts reading a DataFrame. And finishes writing data to exchange
caused by repartition. You can see it here. One task and a shuffle-write operation. The
next stage starts reading the data from the exchange. You can see it here again.
Shuffle read and two tasks. So, this stage runs in parallel because we have two
partitions for this stage. The stage finishes writing data to exchange caused by the
groupBy. You can see the write here. The last stage again reads from the exchange and
executes two parallel tasks. This DAG is for the whole job. If you want to drill down
further into any of the stages, you can go to the stages tab or just click any of the
stages here. Both methods will take you to the same page. Great! That's all for this
video. See you again. Keep Learning and Keep Growing.
