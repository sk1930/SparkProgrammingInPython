Spark is a distributed data processing framework. And typical data processing is a three-step process.

Read the data
Processes it according to your business requirement And
finally, write the outcome of your processing

You can debate it for machine learning, model training, and many other things. However, at a high level, we do these three things, and I am going to start the discussion using these three steps.

So, let's implement the first step in our Spark application. Reading the data. I have a data file named sample.csv. It is a small file with less than ten records, and I am going to use it for local development, debugging, and testing. I am keeping this file in a separate directory named data.

I want to read this file using Spark. So, the first thing that I want to do is to pass the file name and location as a command-line argument to my application. Because I do not want to hardcode the data file name and location in my application. Then we check for the command line argument. If not provided, we log an error and exit the program.

if len(sys.argv) != 2:
        logger.error("Usage: HelloSpark <filename>")
        sys.exit(-1)

surveyDF = spark.read \
	.option("header", "true") \
	.option("inferSchema", "true") \
	.csv(sys.argv[1])


Now, we are ready to read the data file, and the file name is given in the command line argument. Let's do it.


We start with the Spark Session and call the read() method, which returns a DataFrameReader object.

And the DataFrameReader is your gateway to read the data in Apache Spark. It allows you to read the data from a variety of sources, and you can see all these functions here. You can read a CSV, JDBC, JSON, ORC, Parquet, and Text File etc.,.

However, we want to read a CSV file, so let's use the CSV() method. All you need to do is to pass the file path. That's all. Spark will load the data into a Spark DataFrame. CSV data files are plain text data files.

They offer a lot of flexibility, and hence they are complex to read correctly. Let me explain what I mean by CSV being a flexible but complex file format. Look at my data file. The first row is a header row, and the rest of the rows are data rows.

The DataFrameReader doesn't know these details. So, we must tell this information to the DataFrameReader so it can read the file correctly.
You can do it using the DataFrameReader option() method. The option method takes a key-value pair, and you can get a list of all available CSV options from Spark Documentation.

Go to the Spark Home page. Select Documentation. Then choose API docs. --Python ---  Now you can search for dataframe reader.
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html?highlight=dataframereader#pyspark.sql.DataFrameReader

Checkout the CSV method, and you should get the list of all available options. My data file comes with a header row, so I want to set the header option. The default value is false, and I am going to set it to true. Let's do it. Now the DataFrameReader will skip the first row but use it to infer the column names and create a Spark DataFrame.

But What is a DataFrame? Let me give you a quick introduction to the DataFrame. Spark DataFrame is a two-dimensional table-like data structure that is inspired by Pandas DataFrame.
They are a distributed table with named columns and well-defined schema.
That means each column has a specific data type such as integer, float, string, timestamp, etc.
You can visualize your DataFrame as a database table. And most of the operations that you are going to perform on these DataFrames s are also similar to database table operation using rows and columns.

So now you know that a DataFrame must have two things. Column Names And Schema - I mean, data types for each column. Right? Now let's come back to our program. We are reading a CSV file using a DataFrameReader. We are also using the header option.

So, the DataFrameReader is going to use the header row and infer the column names.
But what about the schema? You can use the inferSchema option. This option will allow the DataFrameReader to read a portion of the file and make an intelligent guess about the data types for all the columns. This option works well in some cases, but it fails to infer the correct data types in most of the cases. However, at this stage, we are good to go with the infer schema option. finally return a DataFrame.

However, I do not want to keep this code in my main() method. I recommend that you create a function and move this code to the function in utils.pyspark

Why? Well, we are going to reuse this code at least twice, if not more. The first place is your main() method, and the second place is your unit testing. The point is straight. Break your code into smaller functions that you can reuse and apply unit testing.

I am going to create a function as loadSurveyDF. take two arguments. SparkSession and the file location. And the function is going to return a DataFrame. Paste the code. That's it. Now you can call this function from your main() method. If you want to quickly run it
click on edit configuration
there is a script box
below that is the script parameters

there enter data/sample.csv

and run

and verify your code, use the DataFrame show() method and run it.