48. Spark SQL Engine and Catalyst Optimizer


Welcome Back. We learned to use RDDs, DataFrames, and SQL in Apache Spark.
These are the approaches to process data in Apache Spark.
We do not recommend using RDDs and encourage you to prefer SQL, and DataFrames. And we have a compelling reason for this recommendation.
All these APIs are internally powered by something known as the Spark SQL engine.
The Spark SQL engine is a powerful compiler that optimizes your code and also generates efficient Java Bytecode.
The overall effort of the Spark SQL engine can be broken down into four phases.
The first phase is Analysis. In this phase, the Spark SQL engine will read your code and generate an Abstract Syntax Tree for your SQL or the DataFrame queries.
In this phase, your code is analyzed, and the column names, table, or view names, SQL functions are resolved. You might get a runtime error shown as an analysis error at this stage when your names don't resolve.

The second phase is the logical optimization. In this phase, the SQL engine will apply rule-based optimization and construct a set of multiple execution plans.
Then the catalyst optimizer will use cost-based optimization to assign a cost to each plan. The logical optimization includes standard SQL optimization techniques such as predicate pushdown, projection pruning, boolean expression simplification, and constant folding.
The next phase is the physical planning the SQL engine picks the most effective logical plan and generates a physical plan. The physical plan is nothing but a set of RDD operations, which determines how the plan is going to execute on the Spark cluster.
Finally, the last phase is whole-stage-code-generation. This phase involves generating efficient Java bytecode to run on each machine. This phase was introduced in Spark 2.0 as part of the Project Tungsten. The Project Tungsten was initiated to apply ideas from modern compilers and MPP databases and make Spark run more efficiently. Great! So a lot of things happen inside the Spark engine. However, as a Sark programmer, all we do is to stick to the DataFrame APIs and Spark SQL. That's all. And you will get all these optimization benefits. That's all for this video. Keep Learning and Keep Growing.