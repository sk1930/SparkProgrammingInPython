47. Working with Spark SQL

Welcome Back. You already learned to use Spark RDD, Dataset, and DataFrame APIs in your application. However, the most convenient method to use Spark is via Spark SQL. In this lecture, I am going to create a Spark SQL example. This example will help you understand How to use Spark SQL inside your PySpark application. So, let's start. I have created this new example HelloSparkSQL. Most of the things are the same as our earlier examples. I have this same CSV sample data, log4j.properties, and the lib package to offer the log4j logger. Same as the earlier example, we have the main entry point. I am creating a new SparkSession and loading the data file into a Spark DataFrame. We have done a similar thing in the earlier example. Right? Now we are ready to process the data. You learned to use DataFrame APIs, and you have also seen the RDD API alternative in the earlier videos. In this video, we want to use the SQL expression for doing the same thing. However, you can execute SQL expression only on a table or view. Right? Spark allows you to register your DataFrame as a View. Let me do it. We start with the DataFrame and createOrReplaceTempView. Supply the view name. That's all. You are done. Now you have a view, and you can execute your SQL expressions against this view. Let's do that as well. We start with the SparkSession and use the SQL method. The SQL() method takes your Spark SQL expression and returns a DataFrame. Amazing. Isn't it? All that we did in the earlier examples using multiple lines is done using a single SQL expression. If you know SQL, this approach could be easy to learn and the fastest method to meet your requirements. And the best part, Spark SQL, is as performant as the DataFrames. So, using SQL, you are not going to pay any additional performance cost. That's all for this video. See you again. Keep Learning and Keep Growing.