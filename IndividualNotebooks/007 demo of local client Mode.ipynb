{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19cceee9-8d67-479c-9e4d-20c469fa3d34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first demo for the local client mode\n",
    "cluster --Local\n",
    "Mode -- client mode\n",
    "Tool -- spark shell\n",
    "\n",
    "\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\bin> pyspark --master local[3] --driver-memory 2G\n",
    "\n",
    "\n",
    "And you are going to see how the driver and the executors are getting created in this mode. You will also learn details of using spark-shell. This mode of operation is mostly used by developer community for local Spark development activity. So, let's start with the first demo. You already installed spark binaries on your local machine. I am going to start a command window and run spark-shell in local-client-mode. That's what we wanted to do. But before that, let's look at the spark-shell help.\n",
    "\n",
    "Goto C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\bin\n",
    "pyspark --help\n",
    "\n",
    "\n",
    "So, spark-shell gives you a lot of options. \n",
    "The --master option is to tell the spark-shell about the cluster manager. We want to use the local, and that's the default. However, I am going to use 3 threads. The default star will allow spark-shell to create n number of threads depending upon the resource availability. \n",
    "The deploy mode defaults to the client, and that's what we want. So, we are going to leave that option. You have many other options, but I want to leave them for now and use the --driver-memory. The default JVM heap size is approximately 1GB in most of the cases. And that's what the spark-shell is going to get. For this example, even a 1GB should be more than enough. But if you want to increase the memory for your driver program, then you can use this option. I am going to give it a sum of 2GB. \n",
    "Spark shell gives you many other configuration options. Some are generic, and some are only applicable to the specific cluster managers. For example, you can specify the number of executors, and it is only relevant to the YARN manager. We will use it in the next demo. Great! So we are ready with our spark-shell command. We are going to ask for three threads and a sum of 2GB memory. \n",
    "\n",
    "\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\bin> pyspark --master local[3] --driver-memory 2G\n",
    "\n",
    "\n",
    "The shell is up. Let me do something simple. \n",
    "\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\examples\\src\\main\\resources\\people.json\n",
    "\n",
    ">>>df= spark.read.json(\"C:/demo/.../people.json\")\n",
    ">>>df.show()\n",
    "\n",
    "\n",
    "So I am reading a JSON file and showing the content of the file. Simple. We will learn more coding later. Apache Spark is a well-designed system, and it generates a lot of metrics, logs, and other information to monitor and investigate things about your application. A bunch of information is available to you via Spark Context web UI. And here is the URL. Let's try it and see what we have there. \n",
    "\n",
    "\n",
    "once spark is started it shows the url as \n",
    "Using Python version 3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019 19:37:50)\n",
    "Spark context Web UI available at http://10.108.159.114:4041\n",
    "\n",
    "http://localhost:4040/jobs/\n",
    "or \n",
    "http://localhost:4041/jobs/\n",
    "\n",
    "So you can see the event timeline. Spark started an executor driver process. We do not see the separate driver and executor processes. Why? Because we are in a local cluster, and everything is running inside a single JVM. Right? And the JVM is a combination of driver and executors. And that's what we see here. Just one process which icludes driver as well as the executors. We asked for three threads, and you can see it here in the executors' tab. So all we have is one driver with three cores. And that's the number of threads. And we got roughly 900MB memory for the driver. We asked 2GB, but that's the sum of the overall JVM. Great! So you learned about spark-shell and got some sense of the local cluster, driver, executors, and also saw the Spark Context Web UI. The UI is available only when the spark application is running. You cannot access the Context UI after quitting from the spark-shell. Great! See you again. Keep Learning and Keep Growing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "007 demo of local client Mode",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
