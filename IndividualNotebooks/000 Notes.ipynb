{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fdf45ba-d449-4f1c-99ea-73dcc69e1ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Apache Spark 3 - Spark Programming in Python for Beginners\n",
    "\n",
    "Prashant Kumar Pandey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79dcb1c0-5bf3-4153-abb0-746626e6827c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "25. Execution Methods - How to Run Spark Programs?\n",
    "\n",
    "\n",
    "Welcome Back! By now, you have got some overview of Apache Spark and also got access to various Spark development environments. And I am assuming you can easily answer the following questions. What is Apache Spark? Simple! A distributed computing platform. Right? What do we do with Apache Spark? Again a simple question. We create programs and execute them on a Spark Cluster. Right? The answer to the second question brings two more questions. How do we create Spark Programs? And How do we execute Spark programs on a Spark Cluster? This entire course is all about the first question. You will learn that as we progress with the training. But before we start creating Spark programs, I want to answer the second question. Executing Spark program is not so simple and straightforward. You must have some good sense of Spark Architecure and execution model. So, this entire section is dedicated for understanding Spark Execution Model and the Architecture. Let's start with the straight question. How to execute Spark Programs? We have two methods to run spark programs. Interactive Clients and Submit a Job I already showed you Spark installations. You learned about command-line Spark Shell and web-based Notebooks. Both of these are interactive clients. And they offer you an easy method to run your spark programs. Both of these will allow you to run your Spark code line-by-line and get the output back on your console. Most of the people will use them for learning or during the development phase. Interactive clients are best suitable for exploration purposes. But ultimately, all your exploration will end up into a full-fledged Spark application. Isn't it? You may want to develop a stream processing application or a batch processing job, Spark supports both. For example, you might want to read a news feed as a continuous stream. Then you want to apply a machine-learning algorithm to figure out the type of users that might be interested in each news and redirect the story to those users. You may also want to create a batch job. For example, my YouTube statistics. In this case, we collect the data for 24 hours and start a scheduled spark job to compute the watch-time minutes for the last 24 hours. Finally, the outcome goes to a table and also appears on the dashboard. Right? In both the scenarios, a stream processing Job or a periodic batch job, you must package your application and submit it to the Spark cluster for execution. And that's the second method for executing your programs on a Spark Cluster. For a production use case, you will be using this technique. Apache Spark comes with a spark-submit utility to allow you to submit your Spark jobs to the cluster. We will learn more about it in a separate video, and I will show you how to use spark-submit and various options offered by the tool. Great! So you got your answer to the main question. How to execute Spark Programs? You can run your Spark applications using one of the following methods. Spark shell and Notebooks are the main interactive tools. Whereas the Spark Submit is the primary tool for submitting a Spark program to the cluster. We use interactive clients for learning, development, and exploration. Ã‚ Still, a real-life production implementation is all about packaging your Spark application and submitting it to the cluster for execution. Spark-submit is the most commonly used tool for this purpose. However, most of the Spark vendors are going to offer you some other alternatives. For example, Databricks Cloud will allow you to submit the Notebook itself, and you do not need to package your application and use the spark-submit tool. Most of the cloud-based Spark vendors will also allow you to use Rest APIs or a web-based interface to submit your packaged Spark application. And they internally take care of running the job on the Spark cluster. All those methods are vendor-specific, but spark-submit is a universally accepted method and works in almost all cases. Great! That's all for this lecture. See you again. Keep Learning and Keep growing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2db8dd4-bc5c-4e23-b678-8ef317ce9849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "26. Spark Distributed Processing Model - How your program runs\n",
    "\n",
    "\n",
    "Welcome Back! You already learned that we develop Spark applications and submit them to the Spark Cluster for execution. However, we still don't know how Spark is going to execute our application on a distributed cluster. Spark is a distributed processing engine. Right? So, So, how is the Spark distributed processing model works? And that's the topic for this video. Spark applies a master-slave architecture to every application. So, when we submit our application to the Spark, it is going to create a master process for your application. This master process is then going to create a bunch of slaves to distribute the work and complete your job. You can think of these as runtime containers with some dedicated CPU and memory. In Spark terminology, the master is a driver, and the slaves are the executors. But remember, I am not talking about the cluster. The cluster itself might have a master-node and a bunch of slave nodes. But those things are part of the cluster and are managed by the cluster manager. I am talking about your application and the containers. So, the Spark engine is going to ask for a container from the underlying cluster manager to start the driver process. Once started, the driver is again going to ask for some more containers to start the executor process. And this happens for each application. Let me explain with a simple example. Suppose you are using the spark-submit utility, and you submitted an application A1. Now Spark engine will request the cluster manager to give a container and start a driver process for A1. Once started, the driver will ask for some more containers from the cluster manager and start slave executors. And that's all. Now your driver and executors are responsible for running your application code and doing the job that you wanted. Let's assume you submitted one more application, A2. Now the A2 is going to repeat the same. That means the spark engine will request a container and start the A2 driver. Then the A2 driver is going to ask some more containers and start the A2 executors. And in all this, A1 and A2 are two independent applications. Both of them are following a master-salve architecture and have their own dedicated driver and executors. The point is straight. Every spark application applies a master-slave architecture and runs independently on the cluster. And that is how Spark is a distributed computing platform. Make sense? Great! That's all for this lecture. See you again. Keep Learning and Keep growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8052d0c2-703d-401d-b09d-e8664033683c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b540c2de-5ca5-4f0a-a86d-c9f3e43fc3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "27. Spark Execution Modes and Cluster Managers\n",
    "\n",
    "\n",
    "Welcome Back! In the earlier lecture, I talked about what happens when you submit your application to the Spark cluster. And you learned about the driver and the executors running in containers on the cluster. However, most of the Spark development happens on a local machine or using an interactive tool such as spark-shell or notebooks. So we still have two open questions. How Spark runs your application on a local machine when we do not have a cluster and a cluster manager. And the next question is about how Spark runs our application when we use interactive tools. Because in that case, we do not submit the application to the Spark cluster. This lecture is going to answer both of these questions. Let's start with the first question. How does Spark run on a local machine? You can execute a Spark application on your local machine without even having a real cluster. And we have already done that in an earlier video. Do you remember the Pycharm IDE setup? We executed a HelloSpark program from the IDE. Right? So what happened to the notion of the driver and the executors? Well, you can configure your application to run on a variety of clusters, and the Spark engine is compatible with the following cluster managers. Let me show you. So here is a configuration that tells about the target cluster manager. I used local[3]. In this case, Spark runs locally as a multi-threaded application. In the case of my example, I configured it to start three threads, and that's what the three means. You can have 2, 3, 5, or whatever number you want. If you simply say local and do not give any number, then it becomes a single-threaded application. So, when you configure your application to run with a single local thread, then you will have a driver only and no executors. And in that case, your driver is forced to do everything itself. Nothing happens in parallel. However, when you run your application with three local threads, then your application will have one dedicated thread for the driver and two executor threads. And this local cluster manager is designed for running, testing, and debugging your spark application locally. This technique is nothing but a simulation of distributed client-server architecture using multiple threads so we can test our application locally. Make sense? Great! We will learn more about these other cluster managers(YARN, Kubernetes, Mesos, Standalone) in a later video. Now let's move to the next question. How Does Spark run with an interactive tool? Spark gives you a choice to run your application in one of the following modes. The Client mode is designed for interactive clients . such as spark-shell and the notebooks In this mode, the Spark driver process runs locally at your client machine. However, the driver still connects to the cluster manager and starts all the executors on the cluster. And this is a powerful feature for submitting interactive queries and receiving results back to your client. And that is how your spark-shell and the Notebooks are working. However, if you are quitting your client or you log off from your client machine, then your driver dies, and hence the executors will also die in the absence of the driver. So, client mode is suitable for interactive work but not for long-running jobs. The cluster mode is designed to submit your application to the cluster and let it run. In this mode, everything runs on the cluster. Your driver, as well as the executors. Once you submit your application to run in cluster mode, you can log off from the client machine, and your driver is not impacted because it is running on the cluster. So, the cluster mode is meant for submitting long-running jobs to the cluster. Great! That's all for this lecture. See you again. Keep Learning and keep growing. 0:02\n",
    "1000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f05df1f8-885d-4b09-a3a8-923b7ebba5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "28. Summarizing Spark Execution Models - When to use What?\n",
    "\n",
    "\n",
    "When you are going to run your Spark application, the first thing is to make a decision on which cluster manager you are going to use. There are many options(local[n],YARN, Kuberetes, Mesos, Standalone), but in most of the cases, you will be working with two cluster managers Local and YARN You will be using Local when you are working in your IDE or in a Notebook on your local machine. And you will be using YARN when you are running your Spark application on a real cluster.\n",
    "\n",
    "You have two kinds of the cluster setup. The on-premise cluster which is mostly Cloudera distribution. On-Cloud cluster, which might be Databricks, Google Dataproc, or other similar offerings. Most of them are also using the YARN manager under the hood. \n",
    "\n",
    "Once you know your cluster manager, then you decide on the execution mode. You already learned that Spark supports two modes. Client Mode and the Cluster Mode. You will be using client mode on your local machine with the local cluster manager. Cluster mode doesn't make any sense here because you do not have a real cluster. You can also use the client mode on the YARN cluster when you are working in a YARN cluster, but using a notebook or a spark-shell. Finally, you will be using the cluster mode when you are submitting your Spark application to run on a real cluster. So, we have the three most widely used execution models - client, client mode while using a cluser, cluster.\n",
    "cluster managers  ----Execution modes  -----Execution tools\n",
    "1.local[n]----- client mode ---- IDE, Notebook\n",
    "2. YARN --------client mode on cluster ---- Notebook, shell\n",
    "3. YARN ---- cluster mode ---- spark submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b4d66c1-1220-4002-938b-dee005a14f13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e24ea32c-3540-4340-8fbe-58337aecd0d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "000 Notes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
