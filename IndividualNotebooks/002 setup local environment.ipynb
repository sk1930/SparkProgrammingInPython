{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39cf67f4-7d60-4ae4-aa70-79819e11a642",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. JDK 8 OR 11\n",
    "2. Python 3.6 or higher\n",
    "3. hadoop winutils\n",
    "4. Spark binaries\n",
    "5. Environment Variables\n",
    "6. Python IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd7f1434-c9d1-4016-8c18-a6a72c19e440",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Welcome Back. In this video, I will help you learn to set up your local Spark Development environment. So let's start. Spark set up on your local machine is a complex and lengthy process. It is often difficult for many beginners to achieve this goal. However, nothing to worry about. I will help you learn step by step approach to doing it. Here is the list of things that you need. Let's start with the JDK installation. Spark is a JVM application. It runs in a Java Virtual machine. You might write code in Python, but Spark runs most things in a Java Virtual Machine. So you need JDK on your local machine for installing and running your Spark application. The current most recent Java is at JDK 19. However, as of today, Spark supports JDK 8 or JDK 11. Do not use any other version of JDK because Spark is not well tested on other versions. You might face problems running Spark on other JDK versions. So make sure you have JDK 8 or JDK 11. Let's see how to install JDK 11 on your local machine. Start your browser and visit jdk.java.net. You will a link for the most recent version of the JDK. Click the link to go to the downloads page. Then choose Java SE 11 from the list of all available versions. Download the JDK 11 for your platform. Go to the downloaded file and extract it. You will see the JDK-11 folder. Inside the JDK-11, you will see some more folders. Copy the JDK-11 folder and paste it at some permanent place. I prefer keeping it in c:\\program files\\Java folder. So let me go there. If you already have any other Java versions installed, you might see a Java filter inside the program file. I do not have any other Java version installed on my machine. But I can create a new folder. Let me do it. Now go inside the Java folder and paste your JDK-11. You might need admin rights to paste some folders at this location. If you do not have admin rights, you can paste your JDK-11 at some other location outside the program files. It doesn't matter where do you keep your JDK-11. However, I recommend keeping it inside the C:\\Program Files\\Java folder. Great! You have JDK 11 on your system. It won't work. We have to set up two environment variables to make it work. So let me do that. We have many ways to set up environment variables on a windows machine. But I will show you the command line method for doing the same. Start windows command prompt. Now you can use the setx command to set the JAVA_HOME environment variable. The JAVA_HOME variable must point to your JDK-11 directory. Done. Make sure you see a success message. If you want to check, you can restart the command prompt and check it using the echo command. Let me check it. The second requirement is to add the JAVA_HOME\\bin to your PATH environment variable. Let me do that. You can again use the setx command. Ensure you include the current value of your PATH environment variable and add JAVA_MOME\\bin to the same. If you made a mistake in this command and forgot to add the current value of your PATH variable, you may crash or corrupt your system. So be careful with this command. That's all. I see the success method. You can exit the command prompt, restart it and test your JDK. Let me do that. Execute the Java -version command, and you should see the current Java version. Make sure you see Java version 11. If you already have a Java 8 or Java 11 preinstalled on your machine, you can skip the Java installation steps. However, make sure you are checking the following things. JAVA_HOME environment variable is set, and it is pointing to Java 8 or java 11 JAva_HOME\\bin is included in your PATH environment variable. java -version command is showing JAva 8 or JAva 11 If all these three are there in place, you are done. If not, you must follow the Java installation explained in this video and complete all the steps. Great! The next requirement is to install Python. We will be learning Spark programming in Python. So we also need Python on our local machine. A typical machine comes with Python preinstalled. So it is most likely that you already have Python on your machine. However, That might be old, and I recommend installing the latest Python version. So let me start my browser and search for Python. You might see a link for python.org. Go to the python.org website and download the latest Python version for your machine. Once downloaded, click the file to run it. You should see an option to add Python to your PATH environment variable. Enable that option, so we do not have to do it manually. Then click the install now button. It might take a couple of minutes to complete the installation. You might see this notification to disable the path length limit. An older windows machine allows only 255 characters for any path. The max path length limit is a problem, and this limitation is removed from the newer systems. If you are using an older system, you might see this message. I recommend that you choose this option and remove the max path length. Close the window, and you are done with the Python installation. You should check it once. To start a command prompt and run Python --version command. Make sure you see the latest version that you recently installed. Great! We are done with Python installation. Let's move on to the following requirement. Spark on windows machine also needs winutils.exe and some DLLs. Spark was initially developed to run on Linux-based Hadoop systems. It didn't work on Windows machines. If you try, it throws the following error. You might also see some file permission errors. However, the open-source community created winutils to fix those issues and allow us to run Spark on a windows machine. So we need winutils. Let me download and install it. Start your browser and search for Hadoop winutils. You should see a GitHub page link. Go there. This one is an old repository and is not being actively managed. Scroll down, and you will see a link for the new repository. Follow the link to go to the current repository. Scroll down, and they show you the steps for using this So we need two things. Setup your HADOOP_HOME environment variable Include HADOOP_HOME\\bin to your PATH environment variable. So let me download the repository. Once downloaded, go to the zip file and uncompress it. You will see the winutils-master directory. Go inside, and you will see many other folders. We don't need all of this. So I recommend copying the latest version folder and pasting it at another permanent location. Go inside, and you will see a bin directory. That's all. But we need to set up the environment variables. Right? So start your command prompt and do it. You can use the setx command to set the environment variable. Make sure you see the success message. Now add it to the PATH environment variable. See the success message. Right? That's all, we are done setting up the Hadoop Winutils. Let's move on to the following requirement. We need Spark binaries. So let me start the browser and search for Apache Spark. You should see a link for spark.apache.org. Follow the link to go to the Apache Spark home page. Click the download link. Choose your Spark version. We recommend taking the most recent Spark version. When recording this course, the most recent version is Spark 3.2.1. However, we recommend taking the most recent version even you see a higher version. Everything covered in this course should work with newer versions also. You should also choose the Hadoop version. Choose the most recent version in this drop-down also. Now you can click the download link and start the download. Once downloaded, go to the file and uncompress it. The Spark binary is not a zip file. It is a tgz file. So you might need 7zip for uncompressing this tgz on the windows platform. Let me uncompress it You should see an uncompressed directory. Go inside, and you will see a tar file. This one is also a compressed file. You can untar this file using the 7zip tool. Let me do it. I got another uncompressed directory. Let me go inside it. One more directory layer. Go inside until you see the bin directory. Here it is. I will copy the parent directory of this content. So let me go one level up. The directory name is a little long. You can rename it. Now copy this folder and paste it to a permanent location. Go inside. Great! This is your Spark Home. But we must set the environment variables. So start your command prompt and use the setx command to set the SAPRK_HOME environment variable. Make sure you see the success message. We should also add the SPARK_HOME\\bin to our PATH environment variable. Let me do that. Oops! I see a warning. Data being saved is truncated. You may or may not see this warning. But if you see it, your PATH environment variable is not set correctly. This warning comes on some windows machines where the PATH environment variable is too long. And you cannot use the setx command to add something more to the PATH. However, you can add a more extended PATH environment variable using the Windows UI. Let me show you the steps. Right-click This PC and choose Properties. Go to advanced system settings and then follow the Environment VAriable button. You will see the Path environment variable. Edit it. You can see a long list of things included in the path ENVIRONMENT Variable. Click the new button. Paste the Spark HOme directory location. Also, include the \\bin at the end. That's all. Click Ok and close everything. Ideally, you should be done here setting up Spark on your local machine. Few more patches are required, but Spark should start working on the local machine for most of the students. If not, I will explain some more steps to fix it. However, let me test it on my system and see if My Spark on local started working. Start the command prompt and try running the pyspark shell. Let me do it. You should see some messages and finally a command prompt. IF you see a command prompt and no error messages, you are good to go. You have Apache Spark running on your local machine. You might see a warning stating Exception when trying to compute the page side. But nothing to worry about. Just press enter key, and you will get the command prompt again. You can ignore the warning. Great! So we are almost done setting up Spark on your local machine. However, If you see any error running your PySpark shell, you may want to set up the following environment variables. In fact, I recommend setting up these environment variables even if you do not see any error running your pyspark shell. Setting up these environment variables can save you from seeing unnecessary while working with Spark on your local machine. Start your command prompt and set the PYTHONPATH environment variable. But what is the value? Go back to your Spark home directory. You will see a Python directory there. Go inside. Copy that path and paste it into the PYTHONPATH variable value. But that's not enough. We also need to include one more thing. Let me show you. Go inside the lib directory. You will see a py4j zip file. Copy the path to the zip file. Make sure you are taking the full name, including the .zip extension. Take the absolute path to the py4j zip file and paste it to PYTHONPATH. Done. Let me run it. Worked. Right? We also need to set up the PYSPARK_PYTHON variable. But before that, I need to see my Python installation location. You can use the where-command to find your Python installation location. Let me do that. So I have two Python installations. I installed Python 3.10, but I already had a default Python on my machine. I will configure the Python 3.10 location for my Spark. Let me do that. Done. And I see the success message. Great! Let me retest the pyspark shell once again. It worked. I can see the python command prompt. Right? You are done setting up Spark on your local machine. You can start writing Spark code on the command prompt. But that's pathetic. I mean, We do not want to develop large Spark applications working on the command prompt. I need an IDE for being productive and managing large project development. Right? PyCharm is the most popular IDE for Spark development. So let me start my browser and search for PyCharm. You should see a link for jetbrains.com and follow it. Click the download button and download the community edition. The community edition is a free opensource version of PyCharm. And the community edition is more than enough for our purpose. Execute the downloaded installer and follow the on-screen instruction. Once installed, you can start your PyCharm IDE. Accept the terms and get ready to start your IDE for the first time. You might have an older version of PyCharm. I recommend upgrading it to the latest version. Here is the welcome screen. You may want to customize the look. I like the white theme for recording videos. Maximize the screen. Great! Now come to the Project menu, and you are ready to create your first Spark project. That's all for this video. I will see you in the following video and help you create and run your first Spark program in the IDE. See you again. Keep Learning and Keep Growing. SC\n",
    "Not getting file location\n",
    "0 upvotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a4a95a-701a-4002-96b1-654be2561ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "I had java already installed so didnt install java again\n",
    "ers\\Student>java -version\n",
    "java version \"1.8.0_251\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_251-b08)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)\n",
    "\n",
    "\n",
    "C:\\Program Files\\Java\\jdk1.8.0_251\\bin\n",
    "C:\\Program Files\\Java\\jre1.8.0_251\\bin\n",
    "\n",
    "Python 3.11 didnt work so later installed python 3.8 \n",
    "https://www.python.org/downloads/release/python-380/\n",
    "Windows x86-64 executable installer\n",
    "\n",
    "\n",
    "\n",
    "spark and hadoop versions\n",
    "spark-3.2.4-bin-hadoop3.2 tar file\n",
    "hadoop-3.2.0\n",
    "spark-3.2.4\n",
    "\n",
    "in the end after setting environment variables -- open pycharm if there are more than one python versions installed,\n",
    "To the right bottom click on \n",
    "add interpreter -- local interpreter \n",
    "in location \n",
    "C:\\Users\\Student\\Desktop\\Personal\\Spark\\HelloSpark\\venv1\n",
    "in Base interpreter: \n",
    "    C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n",
    "\n",
    "PYSPARK_PYTHON\n",
    "C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n",
    "\n",
    "PYTHONPATH\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\python\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\\python\\lib\\py4j-0.10.9.5-src.zip\n",
    "\n",
    "\n",
    "SPARK_HOME\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.2.4\n",
    "\n",
    "\n",
    "Path\n",
    "%HADOOP_HOME%\\bin\n",
    "%SPARK_HOME%\\bin\n",
    "%PyCharm Community Edition%\n",
    "C:\\Program Files\\Java\\jdk1.8.0_251\\bin\n",
    "C:\\Program Files\\Java\\jre1.8.0_251\\bin\n",
    "\n",
    "\n",
    "JAVA_HOME - C:\\Program Files\\Java\\jdk1.8.0_251\n",
    "\n",
    "JRE_HOME - C:\\Program Files\\Java\\jre1.8.0_251\n",
    "\n",
    "\n",
    "\n",
    "https://jdk.java.net/java-se-ri/11-MR2 Download jva se 11\n",
    "https://www.python.org/ download python and disbable maximum path limit\n",
    "\n",
    "hadoop winutils -- https://github.com/cdarlint/winutils\n",
    "Click on code - Download Zip - Unzip it \n",
    "You will see the winutils-master directory. Go inside, and you will see many other folders. We don't need all of this. So I recommend copying the latest version folder( here latest was hadoop-3.3.5)\n",
    " and pasting it at another permanent location any drive is fine C:\\Users\\Student\\Desktop\\Personal\\DataBricks\n",
    "\n",
    " . Go inside, and you will see a bin directory. \n",
    "\n",
    "\n",
    "setx HADOOP_HOME=<your local hadoop-ver folder>\n",
    "PATH=%PATH%;%HADOOP_HOME%\\bin\n",
    "\n",
    "on cmd:\n",
    "setx HADOOP_HOME \"C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\hadoop-3.3.5\"\n",
    "\n",
    "\n",
    "setx PATH \"%PATH%;%HADOOP_HOME%\\bin\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "Choose a spark relase -3.5 - choose latest\n",
    "choose package type -- choose latest hadoop - prebuilt for apache hadoop 3.3 and later\n",
    "\n",
    "click on Download Spark: spark-3.5.0-bin-hadoop3.tgz\n",
    "it redirect to a page and asks to click on download again\n",
    "unzip the tgz file and u will get a tar file.\n",
    "extract tar and go inside the folder\n",
    "u will see bin....\n",
    "copy the main folder spark-3.5.0-bin-hadoop3\n",
    "and move it to previous permanent location \n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\n",
    "\n",
    "now C:\\Users\\Student\\Desktop\\Personal\\DataBricks\n",
    " contains hadoop-3.3.5 folder and spark-3.5.0-bin-hadoop3 folder\n",
    " rename the folder as spark-3.5.0\n",
    "\n",
    " setx SPARK_HOME \"C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.5.0\"\n",
    "\n",
    "\n",
    "setx PATH \"%PATH%;%SPARK_HOME%\\bin\"\n",
    "\n",
    "\n",
    "IF U GET ANY WARNING AS DATA BEING SAVED IS TRUNCATED TO 1024 CHARS THEN goto i=environment variables in UI and set the path\n",
    "\n",
    "\n",
    "\n",
    "Goto CMD And run pyspark command\n",
    "exit from pyspark and open cmd again\n",
    "\n",
    "inside spark folder there is a python folder\n",
    "and inside spark/python folder there is a lib folder\n",
    "C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.5.0\\python\\lib\\py4j-0.10.9.7-src.zip\n",
    "\n",
    " add both paths to PYTHONPATH\n",
    "setx PYTHONPATH \"C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.5.0\\python;C:\\Users\\Student\\Desktop\\Personal\\DataBricks\\spark-3.5.0\\python\\lib\\py4j-0.10.9.7-src.zip\"\n",
    "\n",
    "\n",
    "C:\\Users\\Student>where python\n",
    "C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
    "C:\\Users\\Student\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n",
    "\n",
    "C:\\Users\\Student>\n",
    "C:\\Users\\Student>setx PYSPARK_PYTHON \"C:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\"\n",
    "\n",
    "SUCCESS: Specified value was saved.\n",
    "\n",
    "\n",
    "OPEN cmd again and just run pyspark to confirm pyspark is wokring fine\n",
    "\n",
    "\n",
    "Next install pycharm IDE\n",
    "https://www.jetbrains.com/pycharm/\n",
    "\n",
    "IF u scroll down there is a community edition download exe for it\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "002 setup local environment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
