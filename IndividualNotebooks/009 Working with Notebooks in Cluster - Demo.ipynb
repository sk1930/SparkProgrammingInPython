{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad129939-113f-441e-b1bd-d5d62b3d2b57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Cluster --- YARN\n",
    "Mode --- Client Mode\n",
    "Tool : Spark-shell, Notebook\n",
    "\n",
    "\n",
    "Welcome Back! In this lecture, I am going to give you a demo for the YARN client mode using spark-shell as well as the Zeppelin Notebook. This mode of operation is mostly used by data scientists and data analysts for interactive exploration directly in your production cluster. In most cases, you are going to prefer using a notebook for its web-based interface and graph capabilities. However, I will also give you a demo using a spark-shell. So, let's start. I have this 4 node YARN managed cluster. I got one master node, three data nodes, and I have done this setup in the google cloud platform for this demo. This cluster comes with Spark 2.4 because Spark 3.0 is not yet released for production usage. Let me give show you the spark-shell. You already learned to use the spark-shell, right? I am going to ssh to one of these nodes and start the spark-shell. Here is the spark-shell command that we used in the earlier lecture. I am going to change the master to YARN. Reduce the driver's memory to 1GB and ask for the 500 MB executor memory. I am also adding one more configuration to get two executors. You can also add a few more things. Such as how many CPU cores do you need for each executor. Most of the values which I am using here are the default values. So I do not need to tell all these things, and merely running spark-shell will start in client mode with these values. However, I am doing this to help you learn these options. Good. Let's hit the enter key. Great! My spark-shell is now running. My driver and the executors should have already been created, and they will be waiting for me to submit some spark commands. You can access the Spark Context UI and look at the event timeline and other matrices. So, in a real production cluster, we often use the Spark History Server or the YARN resource manager UI. Let me open the Spark History Server. In a secure enterprise setup, you need to ask your operations team to get you access to the Spark History server and also provide the URL link. However, for me, I already know it. Great! So the Spark history server will show you the list of applications that you executed in the past. That's why it is called a history server, right? But you can also get a list of currently running applications. However, It is not showing up for me. That's not a problem. We can get it using YARN resource manager UI. I will show you YARN resource manager in a minute. However, let me stop my PySpark shell and check if I get my application in the Spark History server. I closed the PySpark shell. Let me refresh it. So, here is my application. And the application name is the PySpark Shell. Right? Click the application ID, and it will take you to the Spark Context UI. Let's look at the event timeline. So, we have got one driver and two executors. That's what we asked for. Right? Let's look at the executor tab. So, we now have one driver and two executors. One executor is dead, and that's caused by the YARN dynamic allocation policy. We are not doing anything, and in that case, the YARN dynamic allocation policy is to release the resources. Great! We are done with the spark-shell on a real multi-node YARN cluster. You are not much likely to use this approach. Instead, you would always prefer to use the notebook for your interactive exploration. So, let me show you the same thing with a zeppelin notebook. Let me visit the zeppelin UI. In your secure enterprise setup, you must ask your cluster operations team to provide you the zeppelin URL and also grant you access for the same. I am going to create a notebook. The notebook is not like a shell, which immediately establishes the driver and executors. It is just a web interface which is not yet connected to the cluster. You must run some spark command to initiate the connection. And it could be as simple as checking the spark version. Let me wait until it completes. However, the spark.version executed as a Scala code. Why? Because the default zeppelin notebook cell is a Scala cell. However, We wanted to use PySpark. So, we need to add this line in the begining of the cell. The %pyspark is an interpreter directive for the cell to ensure that the cell is considered as the pyspark cell. Now you can run python or the PySpark code. You can get the list of available interpreter directives from the settings menu. So, you can even use the %sql to execute Spark SQL commands. Now you can go to the spark history server. However, history server is not going to show you the currently running applications. So, what elase can we do? We can go to the YARN resource manager. And it shows you the zeppelin application. Right? Scroll right and click the application master tracking UI. And it should take you to the same Spark UI. So we got one driver and two executors. However, one executor is already released by the YARN dynamic resource allocation policy. You can look at the executor's tab, and you will see similar information. One driver, two executors. However, one executor is already dead. Spark is a highly configurable system. And you can look at the various configuration in the environment tab. So the default zepplin client started with following configurations. spark.executor.instances 2 spark.submit.deployMode client spark.master yarn spark.executor.memory Great! That's all for this lecture. See you again in the next lecture. Keep Learning and Keep Growing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "009 Working with Notebooks in Cluster - Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
