{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb3f147-136c-4e92-9598-9348f407aaca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Welcome Back! We have seen a few things on our local machines. However, Spark is a distributed cluster. So, you may want to get access to a real distributed cluster for exploring how things are going to work on a real distributed cluster. I often use Google Cloud environment to set up a multi-node cluster for exploring Spark in a realistically distributed environment. So, In this lecture, we will set up a multi-node Spark cluster in Google Cloud. I will be using this cluster setup in some of the lectures to give you a demo on how things are going to work in a distributed cluster. So, Let's start. \n",
    "I will be using Google Cloud to set up a Spark cluster. If you want to do it yourself, then you should have a Google Cloud account. It is a paid service, and you will be charged for this. However, Google may offer some free credits for the first time users. You can check the details on the Google Cloud Website. Once you have a GCP account, go to your Console Home page. And you should see a dashboard similar to this. Click the GCP menu and look for the Dataproc. I am going to create a Dataproc cluster. Google Dataproc is an on-demand YARN Cluster which comes with the Spark setup. Give a name to your cluster. Then we select the machine type for our master node. I am taking a machine with 2 CPU and 7.5 GB memory. I don't need a lot of disc space. Now I need to select a machine type for the executor nodes. A single CPU core is good enough for the workers. Reduce the space and create three workers. So, I am going to have a 4 node YARN Cluster having One Master and 3 data nodes. Right? The next important thing is to enable access to web interfaces. This will allow you to access the Spark UI and History server. I will show you that in a minute. We will also get into the advance options and change the Spark Version. So the latest available version is Spark 2.4. Let's take this one. Now we also need some optional components. We are going to use Pyspark so let's take Anaconda. We have already seen Jupyter notebooks in the earlier video. So let me take the Zeppelin notebook this time. The Zeppelin notebook is similar to the Databricks Cloud notebook. If you want, you can schedule the deletion of the cluster. So, the GCP will automatically delete this cluster. Let's create it. Oops! It looks like it is expecting a storage bucket. That's fine. Let me come back and create one storage bucket. Give a name to your bucket. We don't want a multi-region bucket. We want everything to be in the same data center. So, let me select the location. This is the same data center location that I chose for my cluster. I want the storage bucket also to be in the same data center. The rest all is fine. No, we don't want to get into the fine-grained access control. Create the bucket. Come back to the cluster and then create the cluster. Wait for a few minutes, and your cluster is ready. Let me check. So, this one is the dashboard. Here are my 4 VMs, and I can ssh to the master node. I will show you the SSH in a minute. Let's check the web interface. So, you can access the Zeppelin, Spark history server, the YARN resource manager, and a few other things. These are all web-based interfaces, and they are just a click away. I will talk about these when we start using this cluster. Great! So you have a 4 node Spark Cluster. You can use it to explore and learn. When you are done for the day, delete it. Creating and deleting a Spark Cluster in Google cloud is quick and straightforward. So, we create it and use it for 1-2 hours and delete it. If you want to use it again the next day, create a new one, use it, and delete it. That's how you can manage your cost. We also created a storage bucket. Right? Let me go to the home page and check what else I have. So I also have two storage buckets. They are cheaper than the cluster machines, but I don't have a lot of data there, which I want to preserve. So, let me delete them as well. Good. Refresh the page to confirm. Great! So we learned to get access to a distributed spark cluster. I will be using it for the demos, and you can use them for your practice. However, most of the Spark learning happens on your local machine. Great! See you again. Keep Learning and Keep Growing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "008. Installing Multi-Node Spark Cluster - Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
