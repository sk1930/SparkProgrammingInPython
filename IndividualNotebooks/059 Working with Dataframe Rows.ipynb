{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7009012-5eea-4e05-8cfe-ec969749184b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453cd9b0-7bad-42ae-a197-24d83503ecd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def to_date_df(df, fmt, fld):\n",
    "    return df.withColumn(fld, to_date(df[fld], fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80a9fef-d67b-4eb7-b8bc-8d860b1b094f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "my_schema = StructType([\n",
    "  StructField(\"ID\", StringType()),\n",
    "  StructField(\"EventDate\", StringType())])\n",
    "\n",
    "my_rows = [Row(\"123\", \"04/05/2020\"), Row(\"124\", \"4/5/2020\"), Row(\"125\", \"04/5/2020\"), Row(\"126\", \"4/05/2020\")]\n",
    "my_rdd = spark.sparkContext.parallelize(my_rows, 2)\n",
    "my_df = spark.createDataFrame(my_rdd, my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285b3ab3-8ffb-456b-a006-1635f9992c19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ID: string (nullable = true)\n |-- EventDate: string (nullable = true)\n\n+---+----------+\n| ID| EventDate|\n+---+----------+\n|123|04/05/2020|\n|124|  4/5/2020|\n|125| 04/5/2020|\n|126| 4/05/2020|\n+---+----------+\n\n\n After \n\nroot\n |-- ID: string (nullable = true)\n |-- EventDate: date (nullable = true)\n\n+---+----------+\n| ID| EventDate|\n+---+----------+\n|123|2020-04-05|\n|124|2020-04-05|\n|125|2020-04-05|\n|126|2020-04-05|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_df.printSchema()\n",
    "my_df.show()\n",
    "\n",
    "print(\"\\n After \\n\")\n",
    "new_df = to_date_df(my_df,  \"M/d/y\", \"EventDate\")\n",
    "new_df.printSchema()\n",
    "new_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab885623-8be6-4702-a514-b9c6db857226",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "59. Working with Dataframe Rows\n",
    "\n",
    "\n",
    "Welcome Back. Data transformation is all about playing with your DataFrame rows and columns. \n",
    "However, working with a row and column is not that straightforward. \n",
    "\n",
    "In this video, I am going to take you a little deeper into the rows, and we will talk about the columns in the upcoming lecture. So, let's start. \n",
    "\n",
    "Spark DataFrame is a Dataset[Row]. Right? And each row in a DataFrame is a \n",
    "single record represented by an object of type Row. Most of the time, you do \n",
    "not directly work with the entire row. However, we have three specific scenarios when you might have to work with the Row object. Here are them. \n",
    "\n",
    "1. Manually creating Rows and DataFrame. \n",
    "2. Collecting DataFrame rows to the driver. \n",
    "3. Work with an individual row in Spark Transformations. \n",
    "\n",
    "\n",
    "The first two scenarios are mostly used in unit testing or during development. Let me explain with a simple example. However, I want to use the DataBricks cloud for doing this. Why DataBricks Cloud? Well, I want you to be a little more familiar with the DataBricks Cloud environment. No other specific reason. We already learned to create a DataBricks Community Cloud account. Right? \n",
    "\n",
    "Login to your DataBricks Cloud account and create a new cluster. \n",
    "Give a name to your cluster, select your Spark Version, and create it. \n",
    "\n",
    "Good. Now you need a notebook. Let me create a new notebook. Give a name to your notebook. Select your preferred language and create it. Now I am ready to write some code and run it from here. Let's assume I am a spark developer, and I created a function.\n",
    "\n",
    "DataFrame.withColumn(colName, col)[source]\n",
    "\n",
    "Parameters:\n",
    "colNamestr\n",
    "string, name of the new column.\n",
    "\n",
    "colColumn\n",
    "a Column expression for the new column.\n",
    "\n",
    "def to_date_df(df, fmt, fld):\n",
    "    return df.withColumn(fld, to_date(df[fld], fmt))\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "Here it is. This function takes a DataFrame, date format string, and a field name. Then it simply returns a new DataFrame converting the type of the field using a given date format. So, let's assume that the input DataFrame is something like this. And the output DataFrame is expected to be like this. The data type of the EventDate is a string, which is converted to a DateType in the output DataFrame. Now I want to test my function if it works well. Right? Let me import some necessary packages. Your IDE is good at automatically importing things. However, you must take care of these things while working with the Notebooks. Good. Here is my function. How do I test it? I need a DataFrame so I can pass it here and test it. Right? One approach is to create a data file with some sample data. Then you can load it here to create your DataFrame and use that DataFrame to test your function. However, this approach of loading a DataFrame from a file and executing your test cases has got the following problems. A large project might need hundreds of small sample data files to test your functions. This could be difficult to manage over some time. Your build pipeline might run slow due to loading hundreds of sample files and increased I/O. The point is straight. You will be using sample data files for testing your application. However, we prefer to use a well crafted and limited number of sample data files to check some of the critical business scenarios. However, for testing such functions, we are going to create a DataFrame on the fly. Let's do it. A DataFrame requires a schema. So, let me create a schema. Then I create a List of Row Objects. Your list is not distributed. It is a single list with four records. Right? So, I am going to convert it to an RDD of two parallel partitions. Now, you can use this RDD and the Schema to create a DataFrame. Great! So you have your DataFrame. What else? Nothing! Now I can test my function. Let me print the before status. Then I am going to call my function. Finally,y I will print the after state. Let's run it. Great! So My before DataFrame shows a String filed. And the after DataFrame shows a DataType filed. Data also looks good. I have exported this notebook as HTML and attached it to your lecture resources. However, all of this was manual verification. Can you convert it to a Unit test? That's the topic for the next video. See you again. Keep Learning and Keep growing. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "059 Working with Dataframe Rows",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
