39. Data Frame Partitions and Executors


DataFrame is a distributed data structure. In this lecture, I am going to elaborate on how DataFrame is a distributed data structure, and how does it help Spark to implement distributed processing. So, let's start. We already learned that the SparkSession offers you a read() method to read data from a data file such as a CSV file. In a real-life scenario, your CSV file is going to be stored in a distributed storage such as HDFS or cloud storage such as Amazon S3. Right? All these distributed storage systems are designed to partition your data file and store those partitions across the distributed storage nodes. So let's assume that you have these 10 node storage cluster. When you save your file on this cluster, your file might be broken into 100 smaller partitions, and those partitions are stored on these ten nodes. Some storage systems will allow you to control the number of file partitions, but some others may not allow you to control it, and they manage it internally. However, one thing is clear that your data file is stored as smaller partitions, and each storage node may have one or more partitions of your file. Now you are going to read this file using a Spark DataFrameReader. Since the data is already partitioned, so the DataFrameReader is also going to read them as a bunch of in-memory partitions. So you can think of your DataFrame as a bunch of smaller DataFrames, each logically representing a partition. Now let me shift your attention to the Spark driver. I already mentioned that you can think of your Spark session as your Spark driver. Right? So, you told your driver to read the data file. Right? That is why we have this method like spark dot read, and the variable SparkSpark is your driver. Right? So basically, we are telling the driver that we want to read this data file. And the driver is going to reach out to the cluster manager and the storage manager to get the details about the data file partitions. So at runtime, your driver knows how to read the data file and how many partitions are there. Hence, it can create a logical in-memory data structure, which we see as a DataFrame. However, nothing is loaded in the memory yet. Its all a logical structure with enough information to actually load it. So you can think of your DataFrame as a bunch of smaller DataFrames, each logically representing a partition.   Now let me shift your attention to the Spark executors. We already learned that you can configure your SparkSession to start N number of executors. Right? I previously covered it in the earlier lecture, and you can also decide on how much memory and CPU do you want to allocate to your executors. All those configurations are available to your driver. Let's assume that you configured to start five executors, each with 10 GB of memory and 5 CPU cores. Now the driver is again going to reach out to the cluster manager and ask for the containers. Once those containers are allocated, the driver is going to start the executors within those containers. We already know that each executor is nothing but a JVM process with some assigned CPU cores and memory. In this case, each executor JVM is started with 5 CPU cores and 10 GB memory. Right? Now the driver is ready to distribute the work to these executors. So, the driver is going to assign some DataFrame partitions to each JVM core. And these executor cores will load their respective partitions in the memory. And this is how the final state looks like. Now you are ready with a distributed DataFrame setup where each executor core is assigned its own data partition to work on. Isn't it amazing? Yes, it is. And in all this, SparkSpark will also try to minimize the network bandwidth for loading data from the physical storage to the JVM memory. How? Well, that's an internal Spark optimization. While assigning partitions to these executors, SparkSpark will try its best to allocate the partitions which are closest to the executors in the network. However, such data locality is not always possible. And Spark, as well as your cluster manager, is going to work together to achieve the best possible localization. Great! That's all for this lecture. See you again. Keep Learning and Keep Growing.