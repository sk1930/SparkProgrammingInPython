56. Spark Databases and Tables



Welcome Back. In the earlier lectures, you learned about working with file-based data sources and sinks. In this lecture, we are going to learn about the Spark Database and Tables.
So, let's start. Apache Spark is not only a set of APIs and a processing engine.
It is a database in itself. So you can create a database in Spark. Once you have a database, you can create database tables and views.

These tables and views reside inside your database. What does it mean? Well, the table has got two parts. Table Data and Table Metadata.

The table data still resides as data files in your distributed storage by default it is a parquet file. However, you can control the file type.
The metadata is stored in a meta-store called catalog. The meta-store holds the information about the table and its data, such as schema, table name, database name, column names, partitions, the physical location where the actual data resides.
All of this is stored in a central meta-store called catalog. By default, Spark comes with an in-memory catalog which is maintained per spark session.
However, this information goes away when the session ends. So, we needed a persistent and durable meta-store.
Spark decided to reuse the Apache Hive meta-store. We will see all this in an example.
Now let's come back to the Spark Tables.


Spark allows you to create two types of tables Managed Tables and Unmanaged Tables (External Tables) .
For managed tables, Spark manages both: the metadata and the data. What does it mean? Well, that's simple. If you create a managed table, then the Spark is going to do two things. Create and store metadata about the table in the meta-store. Then Spark is going to write the data inside a predefined directory location. This directory is known as the spark.sql.warehouse.dir. So the spark.sql.warehouse.dir is the base location where all your managed tables are stored. And your cluster admin is going to set this base directory location for you. You cannot and should not change it at runtime.
I will show you this as well in the example.


Now let's come to the unmanaged tables. They are also the same with respect to the metadata. However, they are different in terms of data storage location. Let me explain. When you create an unmanaged table, the Spark is going to do only one thing. It will create metadata for your table and store it in the meta-store. That's all.
However, when you create an unmanaged table, then you must specify the data directory location for your table.

You do not have this facility with the managed table. Why? Because the managed table must be stored inside the warehouse directory.

However, unmanaged tables give you the flexibility to store your data at your preferred location. Think of it like this. You already have some data. It is stored at some directory location. And you want to use Spark SQL statements on this dataset, such as select count(*) from ...whatever. However, Spark SQL Engine doesn't know anything about this data. So, you can create an unmanaged table and map the same data to a Spark Table. Now, Spark will create metadata and store it. This will allow you to run your Spark SQL statements on this data. Right?


As a side effect, if you drop your managed table, Spark is going to delete the metadata and the data as well. If you drop an unmanaged table, then Spark is going to remove the metadata and do not touch the data files. And that makes sense as well. Because the unmanaged tables are designed for temporarily mapping your existing data and using it in Spark SQL. Once you are done using it, drop them, and only metadata goes off, leaving your data files untouched.

We prefer using managed tables because they offer some additional features such as bucketing and sorting. All the future improvements in Spark SQL will also target managed tables. Unmanaged tables are external tables, and Spark does not have enough control over them. They are designed for reusing the existing data in Spark SQL, and it should be used in those scenarios only. Great! That's all for this video. In the next video, we will programmatically create a managed table, access the catalog, and also learn to implement bucketing. See you again. Keep Learning and Keep growing.