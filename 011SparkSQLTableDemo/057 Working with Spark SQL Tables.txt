57. Working with Spark SQL Tables


Welcome Back. In this video, we are going to learn to create a managed table and access the catalog. So let's start. I have created this SparkSQLTableDemo example. Most of the things are the same as the earlier example. We are going to create a managed table in the apache spark. Creating managed tables is going to need persistent Metastore. We already learned that the Spark depends on HIve Metastore. So, we need spark-hive for this example. I also created this SparkSQLTableDemo file and the main() method. I am creating SparkSession and loading this DataFrame. You already learned all this.

SparkSession.builder.master("local[3]"). appName("SparkSQLTableDemo").enableHiveSupport().getOrCreate()

We want to use the Hive Metastore, so I am going to enable hive support. This is needed to allow the connectivity to a persistent Hive Metastore. Great! So we are now ready to start working on this example.



What do I want to do? I want to do one simple thing here. Create a managed table and save my DataFrame to the Spark Table. Well, In a real scenario, you will process your data and save the output DataFrame. However, for simplicity, I am not doing any processing here. I just want to learn the mechanics of saving a DataFrame to a Spark Managed table.


However, one question still stands? Why do I want to save my DataFrame as a Managed Table?

Why not as a parquet file. What is the difference? Let me explain. You can save your processed data to a data file such as Parquet or Avro. However, if you want to reaccess it, then you must use the DataFrame Reader API and read it as a DataFrame. Right? However, Spark is a database also. So if you create a managed table in a Spark database, then your data is available to a whole lot of other SQL compliant tools. Spark database tables can be accessed using SQL expressions over JDBC/ODBC connectors. So, you can use other third-party tools such as Tableau, Talend, PowerBI, and a whole bunch of other tools. However, plain data files such as Parquet, Avro, JSON, CSV, etc. are not accessible through JDBC/ODBC interface. Make Sense? Great!

So, let's do it. Writing a DataFrame is as simple as using the DataFrame.write() method. Set the save mode. And finally, saveAsTable(). The saveAsTable() method takes the table name, and it creates a managed table in the current spark database. But what is the current Spark Database? Apache Spark comes with one default database. The database name itself is the default. So, the SaveAsTable() is going to create my flight_data_tbl in the default database. However, I wanted to create this table in the AIRLINE_DB. How to do it? You have two options. Prefix the database name with your table name. Simple. Isn't it? The second option is to access the catalog and set the current database for this session. So I can safely remove the database name from here. Great! We are all set. I am assuming that the AIRLINE_DB already exists. But to be on the safer side, let me create it here. I want to do one thing. The catalog gives you access to the whole bunch of metadata information. Let me use it to get a list of all the tables in my AIRLINE_DB. Good. Let me run it and show you some essential things. Great! You can see the catalog information here. It shows you the table name, database name, and the table type as well. Do you notice these two directories? This one the spark.sql.warehouse.dir. If you look inside, you will see a base directory for your database. Then you will see another subdirectory for the table. And inside your table, you have your data file. Make Sense?
The next one is your persistent metadata store. When you are running in a local machine, these things are created in your current directory. However, in your cluster environment, both of these are configured by your cluster admin. And it would be a common location across all the Spark applications. Great! So I created a Spark SQL table.

However, I wanted to create a partitioned table. So let me add a partitionBy() method. You already learned it earlier. Right? I am using two columns to partition my table. Let me re-run it. Great! So I have overwritten this plain table with a partitioned table. However, I got these 200+ partitions. And this happens because I have 200+ different origins. Right?
For a large dataset, these 200+ partitions are still fine.


However, what if I had 100K unique values for my partition column. Do you want to create 100K partitions? Absolutely not. Right? So, you should not be partitioning your data for a column that has got too many unique values.
Instead, you can use the bucketBy(). Let be do it. The bucketBy() allows you to restrict the number of partitions. I am limiting it to five. So, these five partitions are now called buckets. And I still want to partition my data using the same two columns. I am changing the format to CSV. Parquet is a binary format, and it is the recommended format. However, I want to manually investigate the data, so I am changing it to CSV. Let me run it and see what happens. Great! So now, I got five data files.

These are my five partitions or buckets, whatever you call it. The bucketing does not require a lengthy directory structure. It is as simple as data files.
But how it happened? Let me explain. I asked to create five buckets. So Spark created five files. Each file is one bucket. Now Spark will read one record, look at the key column values. In my case, it is the OP_CARRIER+ORIGIN. Now compute the hash value. Let's say you got some hash number. Divide it by five and take the remainder. You are going to get something between 0 to 4. Right? If the remainder is 0, place the record in the first file. If you got 1, put it in the second file and so on. Right? Well, this is the simplest form of the hashing. There are other variations for hashing, but the basic principle is the same. So you now understand the bucketing. As an end result, each unique key combination is going to produce the same hash value, and hence it is going to land in the same file. Let me show you. So, all the records for the OP_CARRIER=DL and ORIGIN=CLE will land in the first file only. You will not find this combination in any other bucket. Sometimes, these buckets can improve your join operations significantly. However, if these records are sorted, they could be much more ready to use for certain operations. Right? So the bucketBy() can also have a sortBy() companion. Let me rerun it once again. Good! Now, if you look at these files, they are sorted by the same columns. Right? Great! So you learned to create bucketed, sorted, and spark managed tables. That's all for this video. See you again. Keep Learning and Kep Growing.